@article{Ackley1985147,
title = "A learning algorithm for boltzmann machines ",
journal = "Cognitive Science ",
volume = "9",
number = "1",
pages = "147 - 169",
year = "1985",
note = "",
issn = "0364-0213",
%doi = "http://dx.doi.org/10.1016/S0364-0213(85)80012-4",
%url = "http://www.sciencedirect.com/science/article/pii/S0364021385800124",
author = "David H. Ackley, Geoffrey E. Hinton and Terrence J. Sejnowski",
%abstract = "The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure. "
}

@article{Fischer201425,
title = "Training restricted Boltzmann machines: An introduction ",
journal = "Pattern Recognition ",
volume = "47",
number = "1",
pages = "25 - 39",
year = "2014",
note = "",
issn = "0031-3203",
%doi = "http://dx.doi.org/10.1016/j.patcog.2013.05.025",
url = "//www.sciencedirect.com/science/article/pii/S0031320313002495",
author = "Asja Fischer and Christian Igel",
%keywords = "Restricted Boltzmann machines",
%keywords = "Markov random fields",
%keywords = "Markov chains",
%keywords = "Gibbs sampling",
%keywords = "Neural networks",
%keywords = "Contrastive divergence learning",
%keywords = "Parallel tempering ",
%abstract = "Abstract Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. They have attracted much attention as building blocks for the multi-layer learning systems called deep belief networks, and variants and extensions of \{RBMs\} have found application in a wide range of pattern recognition tasks. This tutorial introduces \{RBMs\} from the viewpoint of Markov random fields, starting with the required concepts of undirected graphical models. Different learning algorithms for RBMs, including contrastive divergence learning and parallel tempering, are discussed. As sampling from RBMs, and therefore also most of their learning algorithms, are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and \{MCMC\} techniques is provided. Experiments demonstrate relevant aspects of \{RBM\} training. "
}

@Inbook{Hinton2012,
author="Hinton, Geoffrey E.",
editor="Montavon, Gr{\'e}goire
and Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="A Practical Guide to Training Restricted Boltzmann Machines",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="599--619",
isbn="978-3-642-35289-8",
%doi="10.1007/978-3-642-35289-8_32",
%url="http://dx.doi.org/10.1007/978-3-642-35289-8_32"
}
@incollection{Smolensky87,
  AUTHOR = {P. Smolensky},
  TITLE = {Information Processing in Dynamical Systems: Foundations of Harmony Theory},
  YEAR = 1987,
  BOOKTITLE = {Parallel Distributed Processing: Volume 1: Foundations},
  EDITOR = {D. E. Rumelhart and J. L. McClelland and others},
  PUBLISHER = {MIT Press},
  ADDRESS = {Cambridge},
  PAGES = {194-281},
  KEYWORDS = {}
}

@article{DAI1998159,
title = "Recognition of facial images with low resolution using a hopfield memory model",
journal = "Pattern Recognition",
volume = "31",
number = "2",
pages = "159 - 167",
year = "1998",
note = "",
issn = "0031-3203",
%doi = "http://dx.doi.org/10.1016/S0031-3203(97)00040-X",
url = "http://www.sciencedirect.com/science/article/pii/S003132039700040X",
author = "Ying Dai and Yasuaki Nakano",
%keywords = "Face recognition",
%keywords = "Hopfield model",
%keywords = "Associative memory"
}

@article{SCHULZ1995145,
title = "Simple encoding of infrared spectra for pattern recognition Part 2. Neural network approach using back-propagation and associative Hopfield memory",
journal = "Analytica Chimica Acta",
volume = "316",
number = "2",
pages = "145 - 159",
year = "1995",
note = "",
issn = "0003-2670",
%doi = "http://dx.doi.org/10.1016/0003-2670(95)00353-2",
url = "http://www.sciencedirect.com/science/article/pii/0003267095003532",
author = "Henrik Schulz, Michele Derrick and Dusan Stulik",
%keywords = "Pattern recognition",
%keywords = "Infrared spectrometry",
%keywords = "Back-propagation network",
%keywords = "Content-addressable memory",
%keywords = "Principal component analysis",
%keywords = "Neural networks",
%keywords = "Hopfield model"
}

@inproceedings{KrizhevskyH11,
  added-at = {2014-07-29T00:00:00.000+0200},
  author = {Krizhevsky, Alex and Hinton, Geoffrey E.},
  %biburl = {http://www.bibsonomy.org/bibtex/290401f01f1e5f343bec4b1f2e6734b04/dblp},
  booktitle = {ESANN},
  crossref = {conf/esann/2011}}

@article{hopfield1982neural,
title={Neural networks and physical systems with emergent collective computational abilities},
author={Hopfield, John J},
journal={Proceedings of the national academy of sciences},
volume={79},
number={8},
pages={2554--2558},
year={1982},
publisher={National Acad Sciences}
}

@article{80232, 
author={S. V. B. Aiyer, M. Niranjan and F. Fallside}, 
journal={IEEE Transactions on Neural Networks}, 
title={A theoretical investigation into the performance of the Hopfield model}, 
year={1990}, 
volume={1}, 
number={2}, 
pages={204-215}, 
%keywords={content-addressable storage;eigenvalues and eigenfunctions;matrix algebra;neural nets;operations research;Hopfield model;connection matrix;content-addressable memory;eigenvalues;energy function;hypercube;traveling salesman problem;vector;Associative memory;CADCAM;Computer aided manufacturing;Eigenvalues and eigenfunctions;Equations;Hypercubes;Neurons;Performance analysis;Robustness;Traveling salesman problems}, 
doi={10.1109/72.80232}, 
ISSN={1045-9227}, 
month={Jun}
}

@INPROCEEDINGS{298706, 
author={X. Zhuang and Y. Huang}, 
booktitle={IEEE International Conference on Neural Networks}, 
title={Design of Hopfield content-addressable memories}, 
year={1993}, 
pages={1069-1074 vol.2}, 
%keywords={Hopfield neural nets;content-addressable storage;learning (artificial intelligence);Hamming-stability;Hopfield content-addressable memories;Hopfield neural nets;Rosenblatt's perceptron;attractor;optimal learning rule;Associative memory;CADCAM;Computer aided manufacturing;Content based retrieval;Ear;Information retrieval;Neurons;Performance analysis;Stability;Symmetric matrices}, 
doi={10.1109/ICNN.1993.298706}
}

@INPROCEEDINGS{6889573, 
author={K. Nagatani and M. Hagiwara}, 
booktitle={2014 International Joint Conference on Neural Networks (IJCNN)}, 
title={Restricted Boltzmann machine associative memory}, 
year={2014}, 
pages={3745-3750}, 
%keywords={Boltzmann machines;content-addressable storage;learning (artificial intelligence);RBMAM;conditional probability;contrastive divergence learning procedure;neural network associative memories;noise tolerance;pattern complement;pattern reconstruction calculation;restricted Boltzmann machine associative memory;storage capacity;Associative memory;Biological neural networks;Bit error rate;Hidden Markov models;Noise;Training;Vectors}, 
doi={10.1109/IJCNN.2014.6889573}, 
ISSN={2161-4393}, 
month={July}
}

@ARTICLE{552753, 
author={R. Sammouda, N. Niki and H. Nishitani}, 
journal={IEEE Transactions on Nuclear Science}, 
title={A comparison of Hopfield neural network and Boltzmann machine in segmenting MR images of the brain}, 
year={1996}, 
volume={43}, 
number={6}, 
pages={3361-3369}, 
%keywords={Boltzmann machines;Hopfield neural nets;biomedical NMR;brain;image segmentation;medical image processing;ISODATA clustering technique;MRI;brain MR images segmentation;energy function minimization;global minimum;human brain;local minima;magnetic resonance imaging;medical diagnostic imaging;metastatic tumor;network convergence;squared errors;step function;temporary noise;Artificial neural networks;Convergence;Hopfield neural networks;Image segmentation;Intelligent networks;Magnetic resonance;Magnetic resonance imaging;Pattern recognition;Probability density function;Stability}, 
doi={10.1109/23.552753}, 
ISSN={0018-9499}, 
month={Dec}
}

@article{Barra20121,
title = "On the equivalence of Hopfield networks and Boltzmann Machines ",
journal = "Neural Networks ",
volume = "34",
number = "",
pages = "1 - 9",
year = "2012",
note = "",
issn = "0893-6080",
%doi = "http://dx.doi.org/10.1016/j.neunet.2012.06.003",
url = "//www.sciencedirect.com/science/article/pii/S0893608012001608",
author = "Adriano Barra and Alberto Bernacchia and Enrica Santucci and Pierluigi Contucci",
%keywords = "Statistical mechanics",
%keywords = "Hopfield networks",
%keywords = "Boltzmann Machines ",
%abstract = "A specific type of neural networks, the Restricted Boltzmann Machines (RBM), are implemented for classification and feature detection in machine learning. They are characterized by separate layers of visible and hidden units, which are able to learn efficiently a generative model of the observed data. We study a “hybrid” version of RBMs, in which hidden units are analog and visible units are binary, and we show that thermodynamics of visible units are equivalent to those of a Hopfield network, in which the N visible units are the neurons and the P hidden units are the learned patterns. We apply the method of stochastic stability to derive the thermodynamics of the model, by considering a formal extension of this technique to the case of multiple sets of stored patterns, which may act as a benchmark for the study of correlated sets. Our results imply that simulating the dynamics of a Hopfield network, requiring the update of N neurons and the storage of N ( N − 1 ) / 2 synapses, can be accomplished by a hybrid Boltzmann Machine, requiring the update of N + P neurons but the storage of only N P synapses. In addition, the well known glass transition of the Hopfield network has a counterpart in the Boltzmann Machine: it corresponds to an optimum criterion for selecting the relative sizes of the hidden and visible layers, resolving the trade-off between flexibility and generality of the model. The low storage phase of the Hopfield model corresponds to few hidden units and hence a overly constrained RBM, while the spin-glass phase (too many hidden units) corresponds to unconstrained \{RBM\} prone to overfitting of the observed data. "
}

@article{Agliari201352,
title = "Parallel retrieval of correlated patterns: From Hopfield networks to Boltzmann machines ",
journal = "Neural Networks ",
volume = "38",
number = "",
pages = "52 - 63",
year = "2013",
note = "",
issn = "0893-6080",
%doi = "http://dx.doi.org/10.1016/j.neunet.2012.11.010",
url = "//www.sciencedirect.com/science/article/pii/S0893608012003073",
author = "Elena Agliari and Adriano Barra and Andrea De Antoni and Andrea Galluzzi",
%keywords = "Neural networks",
%keywords = "Boltzmann machines",
%keywords = "Pattern retrieval",
%keywords = "Multitasking networks",
%keywords = "Correlated patterns ",
%abstract = "In this work, we first revise some extensions of the standard Hopfield model in the low storage limit, namely the correlated attractor case and the multitasking case recently introduced by the authors. The former case is based on a modification of the Hebbian prescription, which induces a coupling between consecutive patterns and this effect is tuned by a parameter a . In the latter case, dilution is introduced in pattern entries, in such a way that a fraction d of them is blank. Then, we merge these two extensions to obtain a system able to retrieve several patterns in parallel and the quality of retrieval, encoded by the set of Mattis magnetizations { m μ } , is reminiscent of the correlation among patterns. By tuning the parameters d and a , qualitatively different outputs emerge, ranging from highly hierarchical to symmetric. The investigations are accomplished by means of both numerical simulations and statistical mechanics analysis, properly adapting a novel technique originally developed for spin glasses, i.e. the Hamilton–Jacobi interpolation, with excellent agreement. Finally, we show the thermodynamical equivalence of this associative network with a (restricted) Boltzmann machine and study its stochastic dynamics to obtain even a dynamical picture, perfectly consistent with the static scenario earlier discussed. "
}

@article {Hebb1950,
title = "The organization of behavior: A neuropsychological theory",
editor=  "New York: John Wiley and Sons, Inc.",
journal = "Science Education",
author= "D. O. Hebb",
volume = "34",
number = "5",
publisher = "Wiley Subscription Services, Inc., A Wiley Company",
issn = "1098-237X",
pages = "336--337",
year = "1950"
}

@Inbook{Storkey1997,
author="Storkey, Amos",
editor="Gerstner, Wulfram
and Germond, Alain
and Hasler, Martin
and Nicoud, Jean-Daniel",
title="Increasing the capacity of a hopfield network without sacrificing functionality",
bookTitle="Artificial Neural Networks --- ICANN'97: 7th International Conference Lausanne, Switzerland, October 8--10, 1997 Proceeedings",
year="1997",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="451--456",
isbn="978-3-540-69620-9",
%doi="10.1007/BFb0020196",
%url="http://dx.doi.org/10.1007/BFb0020196"
}

@Inbook{MacKay2003,
  Author = "David J. C. MacKay",
  title = "Hopfield Networks",
  bookitle = "Information Theory, Inference and Learning Algorithms",
  Publisher = "Cambridge University Press",
  Year = "2003",
  ISBN = "0521642981"
}

@inproceedings{carreira2005contrastive,
  title={On Contrastive Divergence Learning.},
  author={Carreira-Perpinan, Miguel A and Hinton, Geoffrey E},
  booktitle={AISTATS},
  volume={10},
  pages={33--40},
  year={2005},
  organization={Citeseer}
}
